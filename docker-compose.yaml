# vim: ft=yaml
version: '3.9'

services:

  postgresql:
    image: bitnami/postgresql:${POSTGRES_VERSION}
    volumes:
      - pgdata:/bitnami/postgresql
    networks:
      openreplay-net:
        aliases:
          - postgresql.db.svc.cluster.local
    environment:
      POSTGRESQL_PASSWORD: "${COMMON_PG_PASSWORD}"
    deploy:
      restart_policy:
        condition: on-failure

  clickhouse:
    image: clickhouse/clickhouse-server:${CLICKHOUSE_VERSION}
    volumes:
      - clickhouse:/var/lib/clickhouse
    networks:
      openreplay-net:
        aliases:
          - clickhouse-openreplay-clickhouse.db.svc.cluster.local
    environment:
      CLICKHOUSE_USER: "default"
      CLICKHOUSE_PASSWORD: ""
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: "1"
    deploy:
      restart_policy:
        condition: on-failure

  redis:
    image: bitnami/redis:${REDIS_VERSION}
    volumes:
      - redisdata:/bitnami/redis/data
    networks:
      openreplay-net:
        aliases:
          - redis-master.db.svc.cluster.local
    environment:
      ALLOW_EMPTY_PASSWORD: "yes"
    deploy:
      restart_policy:
        condition: on-failure

  minio:
    image: bitnami/minio:${MINIO_VERSION}
    volumes:
      - miniodata:/bitnami/minio/data
    networks:
      openreplay-net:
        aliases:
          - minio.db.svc.cluster.local
    ports:
      - target: 9001
        published: 9001
        protocol: tcp
        mode: host
    environment:
      MINIO_ROOT_USER: ${COMMON_S3_KEY}
      MINIO_ROOT_PASSWORD: ${COMMON_S3_SECRET}
    deploy:
      restart_policy:
        condition: on-failure

  fs-permission:
    image: debian:stable-slim
    volumes:
      - shared-volume:/mnt/efs
      - miniodata:/mnt/minio
      - pgdata:/mnt/postgres
    entrypoint: ["/bin/bash", "-c", "chown -R 1001:1001 /mnt/{efs,minio,postgres}"]
    deploy:
      restart_policy:
        condition: on-failure

  minio-migration:
    image: bitnami/minio:2020.10.9-debian-10-r6
    depends_on:
      - minio
      - fs-permission
    networks:
      - openreplay-net
    volumes:
      - ../helmcharts/openreplay/files/minio.sh:/tmp/minio.sh
    environment:
      MINIO_HOST: http://minio.db.svc.cluster.local:9000
      MINIO_ACCESS_KEY: ${COMMON_S3_KEY}
      MINIO_SECRET_KEY: ${COMMON_S3_SECRET}
    user: root
    entrypoint: >
      /bin/bash -c "apt update && apt install netcat -y &&
      until nc -z -v -w30 minio 9000; do
        echo 'Waiting for Minio server to be ready...'; sleep 1;
      done;
      bash /tmp/minio.sh init || exit 100"
    deploy:
      restart_policy:
        condition: on-failure

  db-migration:
    image: bitnami/postgresql:14.5.0
    depends_on:
      - postgresql
      - minio-migration
    networks:
      - openreplay-net
    volumes:
      - ../schema/db/init_dbs/postgresql/init_schema.sql:/tmp/init_schema.sql
    environment:
      PGHOST: postgresql
      PGPORT: 5432
      PGDATABASE: postgres
      PGUSER: postgres
      PGPASSWORD: ${COMMON_PG_PASSWORD}
    entrypoint: >
      /bin/bash -c "until psql -c '\q'; do
        echo 'PostgreSQL is unavailable - sleeping'; sleep 1;
      done;
      echo 'PostgreSQL is up - executing command';
      psql -v ON_ERROR_STOP=1 -f /tmp/init_schema.sql"
    deploy:
      restart_policy:
        condition: on-failure

  clickhouse-migration:
    image: clickhouse/clickhouse-server:${CLICKHOUSE_VERSION}
    depends_on:
      - clickhouse
      - minio-migration
    networks:
      - openreplay-net
    volumes:
      - ../schema/db/init_dbs/clickhouse/create/init_schema.sql:/tmp/init_schema.sql
    environment:
      CH_HOST: "clickhouse-openreplay-clickhouse.db.svc.cluster.local"
      CH_PORT: "9000"
      CH_PORT_HTTP: "8123"
      CH_USERNAME: "default"
      CH_PASSWORD: ""
    entrypoint: >
      /bin/bash -c "until nc -z -v -w30 clickhouse-openreplay-clickhouse.db.svc.cluster.local 9000; do
        echo 'Waiting for Clickhouse server to be ready...'; sleep 1;
      done;
      echo 'Clickhouse is up - executing command';
      clickhouse-client -h clickhouse-openreplay-clickhouse.db.svc.cluster.local --user default --port 9000 --multiquery < /tmp/init_schema.sql || true"
    deploy:
      restart_policy:
        condition: on-failure

  # === App Services ===
  alerts-openreplay:
    image: public.ecr.aws/p1t3u8a3/alerts:${COMMON_VERSION}
    networks:
      openreplay-net:
        aliases:
          - alerts-openreplay
          - alerts-openreplay.app.svc.cluster.local
    volumes:
      - shared-volume:/mnt/efs
    env_file:
      - docker-envs/alerts.env
    deploy:
      restart_policy:
        condition: any

  analytics-openreplay:
    image: public.ecr.aws/p1t3u8a3/analytics:${COMMON_VERSION}
    networks:
      openreplay-net:
        aliases:
          - analytics-openreplay
          - analytics-openreplay.app.svc.cluster.local
    volumes:
      - shared-volume:/mnt/efs
    env_file:
      - docker-envs/analytics.env
    deploy:
      restart_policy:
        condition: any

  # (...) Repete o mesmo padrão para os outros serviços openreplay

  nginx-openreplay:
    image: nginx:latest
    networks:
      - openreplay-net
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
    deploy:
      restart_policy:
        condition: any

  caddy:
    image: caddy:latest
    ports:
      - target: 80
        published: 80
        protocol: tcp
        mode: host
      - target: 443
        published: 443
        protocol: tcp
        mode: host
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    networks:
      - openreplay-net
    environment:
      - ACME_AGREE=true
      - CADDY_DOMAIN=${CADDY_DOMAIN}
    deploy:
      restart_policy:
        condition: any

volumes:
  pgdata:
  clickhouse:
  redisdata:
  miniodata:
  shared-volume:
  caddy_data:
  caddy_config:

networks:
  openreplay-net:
    driver: overlay
